{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset visualization\n",
        "\n",
        "This notebook helps you inspect the raw samples and the corresponding transformed outputs for the datasets supported by the repository. Update the configuration section to point to your local dataset copies before executing the visualization cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to configure a dataset\n",
        "\n",
        "Use the `dataset_str` helper format of the data loader utilities (`DatasetName:key=value:key=value`). Typical arguments for the built-in datasets are:\n",
        "\n",
        "- `ImageNet`: `root`, `extra`, `split` (choose from `TRAIN`, `VAL`, `TEST`).\n",
        "- `ImageNet22k`: `root`, `extra`, `split` (`TRAIN` or `VAL`).\n",
        "- `ADE20K`: `root`, `split` (`TRAIN` or `VAL`).\n",
        "- `CocoCaptions`: `root`, `split` (`TRAIN` or `VAL`).\n",
        "- `NYU`: `root`, `split` (`TRAIN`, `VAL`, or `TEST`).\n",
        "\n",
        "Additional keyword arguments accepted by a dataset class can be appended in the same `key=value` form. The notebook validates that the provided filesystem paths exist before instantiating a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Available transform presets\n",
        "\n",
        "The registry defined below wraps the transformation builders shipped with the repository. Pick one of the preset names or supply your own callable that accepts `(image, target)` and returns `(image, target)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass, replace\n",
        "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from dinov3.data import transforms as data_transforms\n",
        "from dinov3.data.loaders import _parse_dataset_str\n",
        "from dinov3.eval.depth import transforms as depth_transforms\n",
        "from dinov3.eval.segmentation import transforms as seg_transforms\n",
        "\n",
        "logging.getLogger(\"dinov3\").setLevel(logging.WARNING)\n",
        "\n",
        "try:\n",
        "    from torchvision import tv_tensors\n",
        "    TV_TENSOR_TYPES = tuple(\n",
        "        cls\n",
        "        for name, cls in tv_tensors.__dict__.items()\n",
        "        if isinstance(cls, type) and cls.__module__.startswith(\"torchvision.\")\n",
        "    )\n",
        "except Exception:\n",
        "    tv_tensors = None\n",
        "    TV_TENSOR_TYPES: Tuple[type, ...] = tuple()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TransformSpec:\n",
        "    factory: Callable[..., Callable[[Any, Any], Tuple[Any, Any]]]\n",
        "    description: str\n",
        "    denormalize_mean: Optional[Sequence[float]] = None\n",
        "    denormalize_std: Optional[Sequence[float]] = None\n",
        "    value_scale: float = 1.0\n",
        "\n",
        "\n",
        "IMAGENET_MEAN = data_transforms.IMAGENET_DEFAULT_MEAN\n",
        "IMAGENET_STD = data_transforms.IMAGENET_DEFAULT_STD\n",
        "SEGMENTATION_MEAN = tuple(mean * 255.0 for mean in IMAGENET_MEAN)\n",
        "SEGMENTATION_STD = tuple(std * 255.0 for std in IMAGENET_STD)\n",
        "\n",
        "\n",
        "def _identity_factory(**_: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    def _apply(image, target):\n",
        "        return image, target\n",
        "\n",
        "    return _apply\n",
        "\n",
        "\n",
        "def _classification_train_factory(**kwargs: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    image_transform = data_transforms.make_classification_train_transform(**kwargs)\n",
        "\n",
        "    def _apply(image, target):\n",
        "        return image_transform(image), target\n",
        "\n",
        "    return _apply\n",
        "\n",
        "\n",
        "def _classification_eval_factory(**kwargs: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    image_transform = data_transforms.make_classification_eval_transform(**kwargs)\n",
        "\n",
        "    def _apply(image, target):\n",
        "        return image_transform(image), target\n",
        "\n",
        "    return _apply\n",
        "\n",
        "\n",
        "def _segmentation_train_factory(**kwargs: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    return seg_transforms.make_segmentation_train_transforms(**kwargs)\n",
        "\n",
        "\n",
        "def _segmentation_eval_factory(**kwargs: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    return seg_transforms.make_segmentation_eval_transforms(**kwargs)\n",
        "\n",
        "\n",
        "def _depth_train_factory(**kwargs: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    return depth_transforms.make_depth_train_transforms(**kwargs)\n",
        "\n",
        "\n",
        "def _depth_eval_factory(**kwargs: Any) -> Callable[[Any, Any], Tuple[Any, Any]]:\n",
        "    return depth_transforms.make_depth_eval_transforms(**kwargs)\n",
        "\n",
        "\n",
        "TRANSFORM_REGISTRY: Dict[str, TransformSpec] = {\n",
        "    \"identity\": TransformSpec(\n",
        "        factory=_identity_factory,\n",
        "        description=\"Return the original sample without applying any transform.\",\n",
        "    ),\n",
        "    \"classification_train\": TransformSpec(\n",
        "        factory=_classification_train_factory,\n",
        "        description=\"Default classification training pipeline from dinov3.data.transforms.\",\n",
        "        denormalize_mean=IMAGENET_MEAN,\n",
        "        denormalize_std=IMAGENET_STD,\n",
        "    ),\n",
        "    \"classification_eval\": TransformSpec(\n",
        "        factory=_classification_eval_factory,\n",
        "        description=\"Evaluation-time classification preprocessing (resize + center crop).\",\n",
        "        denormalize_mean=IMAGENET_MEAN,\n",
        "        denormalize_std=IMAGENET_STD,\n",
        "    ),\n",
        "    \"segmentation_train\": TransformSpec(\n",
        "        factory=_segmentation_train_factory,\n",
        "        description=\"Segmentation training augmentations from dinov3.eval.segmentation.transforms.\",\n",
        "        denormalize_mean=SEGMENTATION_MEAN,\n",
        "        denormalize_std=SEGMENTATION_STD,\n",
        "        value_scale=255.0,\n",
        "    ),\n",
        "    \"segmentation_eval\": TransformSpec(\n",
        "        factory=_segmentation_eval_factory,\n",
        "        description=\"Segmentation evaluation preprocessing with optional TTA.\",\n",
        "        denormalize_mean=SEGMENTATION_MEAN,\n",
        "        denormalize_std=SEGMENTATION_STD,\n",
        "        value_scale=255.0,\n",
        "    ),\n",
        "    \"depth_train\": TransformSpec(\n",
        "        factory=_depth_train_factory,\n",
        "        description=\"Depth estimation training augmentations from dinov3.eval.depth.transforms.\",\n",
        "        denormalize_mean=IMAGENET_MEAN,\n",
        "        denormalize_std=IMAGENET_STD,\n",
        "    ),\n",
        "    \"depth_eval\": TransformSpec(\n",
        "        factory=_depth_eval_factory,\n",
        "        description=\"Depth estimation evaluation preprocessing (with optional flips).\",\n",
        "        denormalize_mean=IMAGENET_MEAN,\n",
        "        denormalize_std=IMAGENET_STD,\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def resolve_transform(\n",
        "    transform_name: str,\n",
        "    transform_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    *,\n",
        "    custom_transform: Optional[Callable[[Any, Any], Tuple[Any, Any]]] = None,\n",
        "    custom_denorm_overrides: Optional[Dict[str, Any]] = None,\n",
        ") -> Tuple[Callable[[Any, Any], Tuple[Any, Any]], TransformSpec]:\n",
        "    if custom_transform is not None:\n",
        "        overrides = custom_denorm_overrides or {}\n",
        "        spec = TransformSpec(\n",
        "            factory=lambda **_: custom_transform,\n",
        "            description=\"Custom user-supplied transform\",\n",
        "            denormalize_mean=overrides.get(\"mean\"),\n",
        "            denormalize_std=overrides.get(\"std\"),\n",
        "            value_scale=overrides.get(\"value_scale\", 1.0),\n",
        "        )\n",
        "        return custom_transform, spec\n",
        "\n",
        "    if transform_name not in TRANSFORM_REGISTRY:\n",
        "        raise KeyError(f\"Unknown transform preset '{transform_name}'. Available keys: {list(TRANSFORM_REGISTRY)}\")\n",
        "\n",
        "    spec = TRANSFORM_REGISTRY[transform_name]\n",
        "    transform_callable = spec.factory(**(transform_kwargs or {}))\n",
        "    if custom_denorm_overrides:\n",
        "        spec = replace(\n",
        "            spec,\n",
        "            denormalize_mean=custom_denorm_overrides.get(\"mean\", spec.denormalize_mean),\n",
        "            denormalize_std=custom_denorm_overrides.get(\"std\", spec.denormalize_std),\n",
        "            value_scale=custom_denorm_overrides.get(\"value_scale\", spec.value_scale),\n",
        "        )\n",
        "    return transform_callable, spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Available transform presets:\")\n",
        "for name, spec in TRANSFORM_REGISTRY.items():\n",
        "    print(f\" - {name}: {spec.description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# Update the dataset string to match your local setup before running the notebook.\n",
        "DATASET_STR = \"ImageNet:root=/path/to/imagenet:extra=/path/to/imagenet_metadata:split=VAL\"\n",
        "\n",
        "# Select one of the preset transform names defined above.\n",
        "TRANSFORM_NAME = \"classification_eval\"\n",
        "TRANSFORM_KWARGS: Dict[str, Any] = {}\n",
        "\n",
        "# Optional: supply a custom callable transform or override the denormalization parameters.\n",
        "CUSTOM_TRANSFORM = None  # e.g. lambda image, target: (image, target)\n",
        "CUSTOM_DENORMALIZE_OVERRIDES = None  # e.g. {\"mean\": (0.5, 0.5, 0.5), \"std\": (0.5, 0.5, 0.5), \"value_scale\": 1.0}\n",
        "\n",
        "NUM_SAMPLES = 6\n",
        "RANDOM_SEED = 0\n",
        "SHOW_TARGET_SUMMARY = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_dataset_string(dataset_str: str) -> Tuple[type, Dict[str, Any]]:\n",
        "    dataset_cls, kwargs = _parse_dataset_str(dataset_str)\n",
        "    return dataset_cls, dict(kwargs)\n",
        "\n",
        "\n",
        "def validate_dataset_locations(dataset_kwargs: Dict[str, Any]) -> None:\n",
        "    missing = []\n",
        "    for key in (\"root\", \"extra\"):\n",
        "        value = dataset_kwargs.get(key)\n",
        "        if value is None:\n",
        "            continue\n",
        "        if isinstance(value, (str, os.PathLike)):\n",
        "            path_str = os.fspath(value)\n",
        "            if path_str and not os.path.exists(path_str):\n",
        "                missing.append(f\"{key}={path_str}\")\n",
        "    if missing:\n",
        "        joined = \"\\n\".join(missing)\n",
        "        raise FileNotFoundError(\"The following dataset paths could not be located:\n",
        "\" + joined)\n",
        "\n",
        "\n",
        "def instantiate_dataset(dataset_cls: type, dataset_kwargs: Dict[str, Any], transforms: Optional[Callable[[Any, Any], Tuple[Any, Any]]] = None):\n",
        "    init_kwargs = dict(dataset_kwargs)\n",
        "    init_kwargs[\"transforms\"] = transforms\n",
        "    return dataset_cls(**init_kwargs)\n",
        "\n",
        "\n",
        "def ensure_list(obj: Any) -> List[Any]:\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return list(obj)\n",
        "    return [obj]\n",
        "\n",
        "\n",
        "def convert_single_image(\n",
        "    image: Any,\n",
        "    mean: Optional[Sequence[float]] = None,\n",
        "    std: Optional[Sequence[float]] = None,\n",
        "    value_scale: float = 1.0,\n",
        "):\n",
        "    if TV_TENSOR_TYPES and isinstance(image, TV_TENSOR_TYPES):\n",
        "        image = torch.as_tensor(image)\n",
        "\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        tensor = image.detach().cpu()\n",
        "        if tensor.ndim == 4:\n",
        "            return [\n",
        "                convert_single_image(t, mean=mean, std=std, value_scale=value_scale)\n",
        "                for t in tensor\n",
        "            ]\n",
        "        if tensor.ndim == 3:\n",
        "            if mean is not None and std is not None and len(mean) == tensor.shape[0]:\n",
        "                mean_tensor = torch.tensor(mean, dtype=tensor.dtype).view(-1, 1, 1)\n",
        "                std_tensor = torch.tensor(std, dtype=tensor.dtype).view(-1, 1, 1)\n",
        "                tensor = tensor * std_tensor + mean_tensor\n",
        "            if value_scale:\n",
        "                tensor = tensor / float(value_scale)\n",
        "            tensor = tensor.clamp(0.0, 1.0)\n",
        "            array = tensor.permute(1, 2, 0).numpy()\n",
        "            if array.shape[2] == 1:\n",
        "                array = array[:, :, 0]\n",
        "            return array\n",
        "        if tensor.ndim == 2:\n",
        "            array = tensor.numpy()\n",
        "            if value_scale:\n",
        "                array = array / float(value_scale)\n",
        "            return array\n",
        "        if tensor.ndim == 1:\n",
        "            return tensor.numpy()\n",
        "\n",
        "    if isinstance(image, Image.Image):\n",
        "        return np.array(image)\n",
        "\n",
        "    if isinstance(image, np.ndarray):\n",
        "        return image\n",
        "\n",
        "    try:\n",
        "        return np.array(image)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def prepare_images_for_display(\n",
        "    image: Any,\n",
        "    mean: Optional[Sequence[float]] = None,\n",
        "    std: Optional[Sequence[float]] = None,\n",
        "    value_scale: float = 1.0,\n",
        ") -> List[Optional[np.ndarray]]:\n",
        "    prepared: List[Optional[np.ndarray]] = []\n",
        "    for element in ensure_list(image):\n",
        "        converted = convert_single_image(element, mean=mean, std=std, value_scale=value_scale)\n",
        "        if isinstance(converted, list):\n",
        "            prepared.extend(converted)\n",
        "        else:\n",
        "            prepared.append(converted)\n",
        "    return prepared or [None]\n",
        "\n",
        "\n",
        "def summarize_target(target: Any) -> str:\n",
        "    if TV_TENSOR_TYPES and isinstance(target, TV_TENSOR_TYPES):\n",
        "        tensor = torch.as_tensor(target)\n",
        "        return f\"{target.__class__.__name__}{tuple(tensor.shape)}\"\n",
        "    if isinstance(target, torch.Tensor):\n",
        "        return f\"Tensor{tuple(target.shape)} dtype={target.dtype}\"\n",
        "    if isinstance(target, (list, tuple)):\n",
        "        return f\"{type(target).__name__}(len={len(target)})\"\n",
        "    if hasattr(target, \"shape\"):\n",
        "        return f\"{type(target).__name__} shape={getattr(target, 'shape')}\"\n",
        "    return str(target)\n",
        "\n",
        "\n",
        "def visualize_samples(\n",
        "    raw_dataset,\n",
        "    transformed_dataset,\n",
        "    transform_spec: TransformSpec,\n",
        "    indices: List[int],\n",
        "    *,\n",
        "    show_target_summary: bool = True,\n",
        "):\n",
        "    samples = []\n",
        "    max_transformed = 0\n",
        "    for idx in indices:\n",
        "        raw_image, raw_target = raw_dataset[idx]\n",
        "        transformed_image, transformed_target = transformed_dataset[idx]\n",
        "        raw_prepared = prepare_images_for_display(raw_image)\n",
        "        transformed_prepared = prepare_images_for_display(\n",
        "            transformed_image,\n",
        "            mean=transform_spec.denormalize_mean,\n",
        "            std=transform_spec.denormalize_std,\n",
        "            value_scale=transform_spec.value_scale,\n",
        "        )\n",
        "        if not transformed_prepared:\n",
        "            transformed_prepared = [None]\n",
        "        max_transformed = max(max_transformed, len(transformed_prepared))\n",
        "        samples.append((idx, raw_prepared, transformed_prepared, raw_target, transformed_target))\n",
        "\n",
        "    max_transformed = max(max_transformed, 1)\n",
        "    n_rows = len(samples)\n",
        "    n_cols = 1 + max_transformed\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "    if n_rows == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "    if n_cols == 1:\n",
        "        axes = np.expand_dims(axes, axis=1)\n",
        "\n",
        "    for row_idx, (idx, raw_images, transformed_images, raw_target, transformed_target) in enumerate(samples):\n",
        "        row_axes = axes[row_idx]\n",
        "        raw_axis = row_axes[0]\n",
        "        raw_axis.axis(\"off\")\n",
        "        if raw_images and raw_images[0] is not None:\n",
        "            raw_axis.imshow(\n",
        "                raw_images[0],\n",
        "                cmap=\"gray\" if isinstance(raw_images[0], np.ndarray) and raw_images[0].ndim == 2 else None,\n",
        "            )\n",
        "        else:\n",
        "            raw_axis.text(0.5, 0.5, \"No preview\", ha=\"center\", va=\"center\")\n",
        "        raw_axis.set_title(f\"Raw (idx={idx})\")\n",
        "        if show_target_summary:\n",
        "            raw_axis.set_xlabel(summarize_target(raw_target))\n",
        "\n",
        "        for offset in range(max_transformed):\n",
        "            axis = row_axes[offset + 1]\n",
        "            axis.axis(\"off\")\n",
        "            if offset < len(transformed_images) and transformed_images[offset] is not None:\n",
        "                image_arr = transformed_images[offset]\n",
        "                axis.imshow(\n",
        "                    image_arr,\n",
        "                    cmap=\"gray\" if isinstance(image_arr, np.ndarray) and image_arr.ndim == 2 else None,\n",
        "                )\n",
        "            else:\n",
        "                axis.text(0.5, 0.5, \"No preview\", ha=\"center\", va=\"center\")\n",
        "            suffix = \"\" if offset == 0 else f\" #{offset + 1}\"\n",
        "            axis.set_title(f\"Transformed{suffix}\")\n",
        "            if show_target_summary and offset == 0:\n",
        "                axis.set_xlabel(summarize_target(transformed_target))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_cls, dataset_kwargs = parse_dataset_string(DATASET_STR)\n",
        "validate_dataset_locations(dataset_kwargs)\n",
        "transform_callable, transform_spec = resolve_transform(\n",
        "    TRANSFORM_NAME,\n",
        "    TRANSFORM_KWARGS,\n",
        "    custom_transform=CUSTOM_TRANSFORM,\n",
        "    custom_denorm_overrides=CUSTOM_DENORMALIZE_OVERRIDES,\n",
        ")\n",
        "\n",
        "raw_dataset = instantiate_dataset(dataset_cls, dataset_kwargs, transforms=None)\n",
        "transformed_dataset = instantiate_dataset(dataset_cls, dataset_kwargs, transforms=transform_callable)\n",
        "\n",
        "print(f\"Dataset class: {dataset_cls.__name__}\")\n",
        "for key, value in dataset_kwargs.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"Transform preset: {TRANSFORM_NAME}\")\n",
        "if transform_spec.denormalize_mean is not None:\n",
        "    print(f\"  Denormalize mean: {transform_spec.denormalize_mean}\")\n",
        "if transform_spec.denormalize_std is not None:\n",
        "    print(f\"  Denormalize std: {transform_spec.denormalize_std}\")\n",
        "print(f\"  Value scale: {transform_spec.value_scale}\")\n",
        "\n",
        "num_available = len(raw_dataset)\n",
        "if num_available == 0:\n",
        "    raise RuntimeError(\"The dataset is empty; nothing to visualize.\")\n",
        "\n",
        "num_to_show = min(NUM_SAMPLES, num_available)\n",
        "if num_to_show < NUM_SAMPLES:\n",
        "    print(f\"Requested {NUM_SAMPLES} samples but only {num_available} are available. Showing {num_to_show}.\")\n",
        "\n",
        "indices = random.Random(RANDOM_SEED).sample(range(num_available), k=num_to_show) if num_available >= num_to_show else list(range(num_available))\n",
        "indices.sort()\n",
        "print(f\"Visualizing indices: {indices}\")\n",
        "\n",
        "fig = visualize_samples(\n",
        "    raw_dataset,\n",
        "    transformed_dataset,\n",
        "    transform_spec,\n",
        "    indices,\n",
        "    show_target_summary=SHOW_TARGET_SUMMARY,\n",
        ")\n",
        "fig\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
